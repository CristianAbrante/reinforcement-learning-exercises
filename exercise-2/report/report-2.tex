\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[many]{tcolorbox}
\tcbuselibrary{listings}
\usepackage{listings}
\usepackage{float}

\definecolor{lg}{HTML}{f0f0f0}

\newtcblisting{pycode}{
    colback=lg,
    boxrule=0pt,
    arc=0pt,
    outer arc=0pt,
    top=0pt,
    bottom=0pt,
    colframe=white,
    listing only,
    left=15.5pt,
    enhanced,
    listing options={
        basicstyle=\small\ttfamily,
        keywordstyle=\color{blue},
        language=Python,
        showstringspaces=false,
        tabsize=2,
        numbers=left,
        breaklines=true
    },
    overlay={
        \fill[gray!30]
        ([xshift=-3pt]frame.south west)
        rectangle
        ([xshift=11.5pt]frame.north west);
    }
}

\lstset{
    language=Python,
    basicstyle=\small\ttfamily,
}

 
\begin{document}
 
\title{Exercise 2}
\author{Cristian Manuel Abrante Dorta - 888248\\
ELEC-E8125 - Reinforcement Learning}

\maketitle
\section{Sailor gridword}

\subsection{Question 1: What is the agent and the environment in this setup?}

The problem of the sailor gridworld could be formally described using a Markov Decision process. In this case, the agent is the sailor on the boat, which objective is to arrive with the boat to the secure harbour.\\

Apart from that, we can define the environment as a set $S$ of states, where each state is the position $(x,y)$ on a squared grid.

\begin{equation}
    S = {s_{i,j} = s(i,j) | i \in [0, N], j\in [0,N]}
\end{equation}

This environment have terminal positions or states: such as the harbour and the rocks (with different reward depending on them), and non terminal positions, such as the sea. When the agent reach a terminal position, the episode will finish, and a reward would be obtained, depending on the finish  state reached. \\

Aditionally, the agent can take a set of different actions ($A$) in each state:

\begin{equation}
    A = \{r, l, u, d\}
\end{equation}

Probabilities of arriving to a certain state giving one action and a current state are not constant over the environment, having some increased level of randomness in the zone defined by strong winds.

\section{Value iteration}

\subsection{Task 1: Implement value iteration for the sailor example.}

After implementing value iteration algorithm, we plot the board for observing the results of the value state function and the optimal action for each state (Figure \ref{fig:final-board}).

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.25]{exercise-2/report/img/final-board.png}
    \caption{Board with value function for each state, after iteration 100}
    \label{fig:final-board}
\end{figure}

\subsection{Question 2: What is the state value of the harbour and rock states? Why?}

The state-value function both for the rocks and for the harbour have the value of zero, this happens because those states are final states, so the episode finish if the agent reach them. And that also makes sense if we consider the value function as the expected reward when starting in state $s$, because if that state when we are \textit{starting} is final, then we could not expect more value.

\subsection{Task 2: compute the optimal policy}

Running the same value iteration algorithm as before, the optimal policy could be computed, as the action which gives us the best expected reward, and we can plot it in the similar board as the previous sections (Figure \ref{fig:pol-final-board}):

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.25]{exercise-2/report/img/final-board-with-policy.png}
    \caption{Board with value function and policy for each state, after iteration 100}
    \label{fig:pol-final-board}
\end{figure}

\subsection{Question 3: Which path did the sailor choose, the safe path below the rocks, or the dangerous path between the rocks? If you change the reward for hitting the rocks to -10 (that is, make the sailor value life more), does he still choose the same path?}

I have made five different executions of the program, and in all the cases, the agent decides to take the risky path between the rocks, achieving the goal in 3 times out of five. It it reasonable that the sailor decides to go between the rocks, because the value function for the sea states in the middle of the rocks is bigger than the value of the surrounding states. \\

If the reward value of the rock states is changed to -10, then the value functions for each states are updated accordingly:

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.25]{exercise-2/report/img/final-board-with-policy.png}
    \caption{Board with value function and policy for each state, after iteration 100}
    \label{fig:pol-final-board}
\end{figure}

\bibliographystyle{ieeetr}
\bibliography{template}  % Modify template with your bibliography name
\end{document}
