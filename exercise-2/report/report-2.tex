\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[many]{tcolorbox}
\tcbuselibrary{listings}
\usepackage{listings}
\usepackage{float}

\definecolor{lg}{HTML}{f0f0f0}

\newtcblisting{pycode}{
    colback=lg,
    boxrule=0pt,
    arc=0pt,
    outer arc=0pt,
    top=0pt,
    bottom=0pt,
    colframe=white,
    listing only,
    left=15.5pt,
    enhanced,
    listing options={
        basicstyle=\small\ttfamily,
        keywordstyle=\color{blue},
        language=Python,
        showstringspaces=false,
        tabsize=2,
        numbers=left,
        breaklines=true
    },
    overlay={
        \fill[gray!30]
        ([xshift=-3pt]frame.south west)
        rectangle
        ([xshift=11.5pt]frame.north west);
    }
}

\lstset{
    language=Python,
    basicstyle=\small\ttfamily,
}

 
\begin{document}
 
\title{Exercise 2}
\author{Cristian Manuel Abrante Dorta - 888248\\
ELEC-E8125 - Reinforcement Learning}

\maketitle
\section{Sailor gridword}

\subsection{Question 1: What is the agent and the environment in this setup?}

The problem of the sailor gridworld could be formally described using a Markov Decision process. In this case, the agent is the sailor on the boat, which objective is to arrive with the boat to the secure harbour.\\

Apart from that, we can define the environment as a set $S$ of states, where each state is the position $(x,y)$ on a squared grid.

\begin{equation}
    S = {s_{i,j} = s(i,j) | i \in [0, N], j\in [0,N]}
\end{equation}

This environment have terminal positions or states: such as the harbour and the rocks (with different reward depending on them), and non terminal positions, such as the sea. When the agent reach a terminal position, the episode will finish, and a reward would be obtained, depending on the finish  state reached. \\

Aditionally, the agent can take a set of different actions ($A$) in each state:

\begin{equation}
    A = \{r, l, u, d\}
\end{equation}

Probabilities of arriving to a certain state giving one action and a current state are not constant over the environment, having some increased level of randomness in the zone defined by strong winds.

\section{Value iteration}

\subsection{Task 1: Implement value iteration for the sailor example.}

After implementing value iteration algorithm, we plot the board for observing the results of the value state function and the optimal action for each state (Figure \ref{fig:final-board}).

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.25]{exercise-2/report/img/final-board.png}
    \caption{Board with value function for each state, after iteration 100}
    \label{fig:final-board}
\end{figure}

\subsection{Question 2: What is the state value of the harbour and rock states? Why?}

The state-value function both for the rocks and for the harbour have the value of zero, this happens because those states are final states, so the episode finish if the agent reach them. And that also makes sense if we consider the value function as the expected reward when starting in state $s$, because if that state when we are \textit{starting} is final, then we could not expect more value.

\subsection{Task 2: compute the optimal policy}

Running the same value iteration algorithm as before, the optimal policy could be computed, as the action which gives us the best expected reward, and we can plot it in the similar board as the previous sections (Figure \ref{fig:pol-final-board}):

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.25]{exercise-2/report/img/final-board-with-policy.png}
    \caption{Board with value function and policy for each state, after iteration 100}
    \label{fig:pol-final-board}
\end{figure}

\subsection{Question 3: Which path did the sailor choose, the safe path below the rocks, or the dangerous path between the rocks? If you change the reward for hitting the rocks to -10 (that is, make the sailor value life more), does he still choose the same path?}

I have made five different executions of the program, and in all the cases, the agent decides to take the risky path between the rocks, achieving the goal in 3 times out of five. It it reasonable that the sailor decides to go between the rocks, because the value function for the sea states in the middle of the rocks is bigger than the value of the surrounding states. \\

If the reward value of the rock states is changed to -10, then the value functions for each states are updated accordingly:

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.25]{exercise-2/report/img/final-board-with-policy-2.png}
    \caption{Board with reward -10 for the rocks after iteration 100}
    \label{fig:pol-final-board}
\end{figure}

The execution in this board have a different result, because in this case the agent does not take so much risk, and decides to follow the safe path through the bottom of the board. 

\subsection{Question 4: What happens if you run the algorithm for a smaller amount of iterations?}

We have created five different executions for smaller number of iterations in order to see if the algorithm converges, and those results are presented in the following table

\begin{table}[h]
\centering
\begin{tabular}{l|l}
         & converges \\ \hline
max = 10 & False     \\
max = 25 & False     \\
max = 50 & True      \\
max = 75 & True      \\
max = 80 & True     
\end{tabular}
\end{table}

As we can see for small number of maximum iterations the algorithm does not converge. When comparing value function with policy, we can say that policy takes less time to converge, because the function of the values is numeric so the conditions for convergence are more strict and they require more iterations.

\subsection{Task 3: Set the reward for crashing into the rocks back to -2. Change the termination condition of your algorithm to make it run until convergence.}

Setting the convergence criteria for the algorithm, produces the following result in the final board:

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.25]{exercise-2/report/img/final-board-with-threshold.png}
    \caption{Board with convergence criteria after iteration 35}
    \label{fig:pol-final-board}
\end{figure}

In this case, the algorithm converges after \textbf{35 iterations}.

\subsection{Task 4: Modify the program to compute the discounted return of the initial state}

After the execution of the program (with convergence criteria for value iteration), those were the obtained results for the discounted return of the initial state:

\begin{itemize}
    \item $\Bar{G} = 0.6251$
    \item $std(G) = 1.3645$
\end{itemize}

\subsection{Question 5: What is the relationship between the discounted return and the value function? Explain briefly.}

Value function for each state is defined as the expected return if the agent is located in this state and follows a policy $\pi$, which can be represented mathematically as:

\begin{equation}
    v_\pi(s) = E[G | S_t= s]
\end{equation}

And the discounted return is the actual value obtained when all the iterations have been executed, is the quantity that the value function tries to estimate. \\

We can test if the value function expected and the one obtained were accurate for our particular case. And we can see that they actually are, because we have calculated that the expected return is 0.66 with value iteration and the real calculation was 0.6251, which differs roughly in 0.04 units.

\subsection{Question 6: s Imagine a reinforcement learning problem involving a robot exploring an unknown environment.}

One of the key characteristics of this problem is that the environment is fully known, so before starting the path, we can iterate thought all the states in order to calculate the value function for each one. In the case of an unknown environment, we can not proceed in the same way because we do not know the dinamics of the environment.


\end{document}
