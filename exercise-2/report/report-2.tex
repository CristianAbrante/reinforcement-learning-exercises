\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[many]{tcolorbox}
\tcbuselibrary{listings}
\usepackage{listings}

\definecolor{lg}{HTML}{f0f0f0}

\newtcblisting{pycode}{
    colback=lg,
    boxrule=0pt,
    arc=0pt,
    outer arc=0pt,
    top=0pt,
    bottom=0pt,
    colframe=white,
    listing only,
    left=15.5pt,
    enhanced,
    listing options={
        basicstyle=\small\ttfamily,
        keywordstyle=\color{blue},
        language=Python,
        showstringspaces=false,
        tabsize=2,
        numbers=left,
        breaklines=true
    },
    overlay={
        \fill[gray!30]
        ([xshift=-3pt]frame.south west)
        rectangle
        ([xshift=11.5pt]frame.north west);
    }
}

\lstset{
    language=Python,
    basicstyle=\small\ttfamily,
}

 
\begin{document}
 
\title{Exercise 2}
\author{Cristian Manuel Abrante Dorta - 888248\\
ELEC-E8125 - Reinforcement Learning}

\maketitle
\section{Sailor gridword}

\subsection{Question 1: What is the agent and the environment in this setup?}

The problem of the sailor gridworld could be formally described using a Markov Decision process. In this case, the agent is the sailor on the boat, which objective is to arrive with the boat to the secure harbour.\\

Apart from that, we can define the environment as a set $S$ of states, where each state is the position $(x,y)$ on a squared grid.

\begin{equation}
    S = {s_{i,j} = s(i,j) | i \in [0, N], j\in [0,N]}
\end{equation}

This environment have terminal positions or states: such as the harbour and the rocks (with different reward depending on them), and non terminal positions, such as the sea. When the agent reach a terminal position, the episode will finish, and a reward would be obtained, depending on the finish  state reached. \\

Aditionally, the agent can take a set of different actions ($A$) in each state:

\begin{equation}
    A = \{r, l, u, d\}
\end{equation}

Probabilities of arriving to a certain state giving one action and a current state are not constant over the environment, having some increased level of randomness in the zone defined by strong winds.

\section{Value iteration}

\subsection{Implement value iteration for the sailor example.}

The value iteration for updating the values and optimal policy for each state of the grid gave us the following result for the final iteration:

\begin{figure}
    \centering
    \includegraphics{}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\bibliographystyle{ieeetr}
\bibliography{template}  % Modify template with your bibliography name
\end{document}
